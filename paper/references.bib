
@misc{center_for_history_and_new_media_zotero_nodate,
	title = {Zotero {Quick} {Start} {Guide}},
	url = {http://zotero.org/support/quick_start_guide},
	author = {{Center for History and New Media}}
}

@inproceedings{minka_expectation_2001,
	title = {Expectation propagation for approximate {Bayesian} inference},
	url = {http://dl.acm.org/citation.cfm?id=2074067},
	urldate = {2017-01-21},
	booktitle = {Proceedings of the {Seventeenth} conference on {Uncertainty} in artificial intelligence},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Minka, Thomas},
	year = {2001},
	pages = {362--369},
	file = {1301.2294.pdf:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\TWFMEW3E\\1301.2294.pdf:application/pdf}
}

@article{kuss_assessing_2005,
	title = {Assessing approximate inference for binary {Gaussian} process classification},
	volume = {6},
	url = {http://www.jmlr.org/papers/v6/kuss05a.html},
	number = {Oct},
	urldate = {2017-05-23},
	journal = {Journal of machine learning research},
	author = {Kuss, Malte and Rasmussen, Carl Edward},
	year = {2005},
	pages = {1679--1704},
	file = {kuss05a.pdf:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\6TP9P5N3\\kuss05a.pdf:application/pdf}
}

@article{ruozzi_bethe_2012,
	title = {The {Bethe} {Partition} {Function} of {Log}-supermodular {Graphical} {Models}},
	url = {http://arxiv.org/abs/1202.6035},
	abstract = {Sudderth, Wainwright, and Willsky have conjectured that the Bethe approximation corresponding to any fixed point of the belief propagation algorithm over an attractive, pairwise binary graphical model provides a lower bound on the true partition function. In this work, we resolve this conjecture in the affirmative by demonstrating that, for any graphical model with binary variables whose potential functions (not necessarily pairwise) are all log-supermodular, the Bethe partition function always lower bounds the true partition function. The proof of this result follows from a new variant of the "four functions" theorem that may be of independent interest.},
	urldate = {2017-05-23},
	journal = {arXiv:1202.6035 [math-ph]},
	author = {Ruozzi, Nicholas},
	month = feb,
	year = {2012},
	note = {arXiv: 1202.6035},
	keywords = {Computer Science - Discrete Mathematics, Mathematical Physics, Mathematics - Combinatorics},
	file = {arXiv\:1202.6035 PDF:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\KXSFC779\\Ruozzi - 2012 - The Bethe Partition Function of Log-supermodular G.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\6V23FBDQ\\1202.html:text/html}
}

@inproceedings{willsky_loop_2008,
	title = {Loop series and {Bethe} variational bounds in attractive graphical models},
	url = {http://papers.nips.cc/paper/3354-loop-series-and-bethe-variational-bounds-in-attractive-graphical-models},
	urldate = {2017-05-23},
	booktitle = {Advances in neural information processing systems},
	author = {Willsky, Alan S. and Sudderth, Erik B. and Wainwright, Martin J.},
	year = {2008},
	pages = {1425--1432},
	file = {SudWaiWil07.pdf:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\EGP8DV29\\SudWaiWil07.pdf:application/pdf}
}

@inproceedings{weller_clamping_2014,
	title = {Clamping variables and approximate inference},
	url = {http://papers.nips.cc/paper/5529-clamping-variables-and-approximate-inference},
	urldate = {2017-05-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Weller, Adrian and Jebara, Tony},
	year = {2014},
	pages = {909--917},
	file = {NIPS14-clamp.pdf:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\PMBV8C44\\NIPS14-clamp.pdf:application/pdf}
}

@article{cunningham_gaussian_2011,
	title = {Gaussian {Probabilities} and {Expectation} {Propagation}},
	url = {http://arxiv.org/abs/1111.6832},
	abstract = {While Gaussian probability densities are omnipresent in applied mathematics, Gaussian cumulative probabilities are hard to calculate in any but the univariate case. We study the utility of Expectation Propagation (EP) as an approximate integration method for this problem. For rectangular integration regions, the approximation is highly accurate. We also extend the derivations to the more general case of polyhedral integration regions. However, we find that in this polyhedral case, EP's answer, though often accurate, can be almost arbitrarily wrong. We consider these unexpected results empirically and theoretically, both for the problem of Gaussian probabilities and for EP more generally. These results elucidate an interesting and non-obvious feature of EP not yet studied in detail.},
	urldate = {2017-05-23},
	journal = {arXiv:1111.6832 [stat]},
	author = {Cunningham, John P. and Hennig, Philipp and Lacoste-Julien, Simon},
	month = nov,
	year = {2011},
	note = {arXiv: 1111.6832},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1111.6832 PDF:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\XRT9GKSJ\\Cunningham et al. - 2011 - Gaussian Probabilities and Expectation Propagation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\GWS4PXXM\\1111.html:text/html}
}

@phdthesis{minka_family_2001,
	title = {A family of algorithms for approximate {Bayesian} inference},
	url = {https://dspace.mit.edu/bitstream/handle/1721.1/86583/48118181-MIT.pdf?sequence=2},
	urldate = {2017-05-23},
	school = {Massachusetts Institute of Technology},
	author = {Minka, Thomas Peter},
	year = {2001},
	file = {48118181-MIT.pdf:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\APMG86KH\\48118181-MIT.pdf:application/pdf;Dehaene Phd-3.pdf:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\KM72KFHI\\Dehaene Phd-3.pdf:application/pdf}
}

@techreport{minka_divergence_2005,
	title = {Divergence measures and message passing},
	url = {https://www.seas.harvard.edu/courses/cs281/papers/minka-divergence.pdf},
	urldate = {2017-05-23},
	institution = {Technical report, Microsoft Research},
	author = {Minka, Tom},
	year = {2005},
	file = {tr-2005-173.pdf:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\7JCJS6JK\\tr-2005-173.pdf:application/pdf}
}

@techreport{minka_power_2004,
	title = {Power ep},
	url = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2004-149.pdf},
	urldate = {2017-05-23},
	institution = {Technical report, Microsoft Research, Cambridge},
	author = {Minka, Thomas},
	year = {2004},
	file = {tr-2004-149.pdf:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\WA2FSVB3\\tr-2004-149.pdf:application/pdf}
}

@phdthesis{dehaene_statisticien_2016,
	title = {Le statisticien neuronal},
	school = {Paris Descartes University},
	author = {Dehaene, Guillaume},
	month = sep,
	year = {2016},
	file = {Dehaene Phd-3.pdf:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\KD9RXWRV\\Dehaene Phd-3.pdf:application/pdf}
}

@misc{cunningham_expectation_2015,
	address = {Columbia University},
	title = {Expectation {Propagation}: {Factorization} and {Entropy} {Approximation}},
	url = {http://gpss.cc/gpa15/assets/cunningham.pdf},
	urldate = {2017-05-23},
	author = {Cunningham, John P.},
	month = may,
	year = {2015},
	file = {cunningham.pdf:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\TD92ACDC\\cunningham.pdf:application/pdf}
}

@misc{paquet_towards_nodate,
	title = {Towards (?) marginal likelihood lower bounds with {EP}},
	url = {http://gpss.cc/gpa15/assets/paquet.pdf},
	urldate = {2017-05-23},
	author = {Paquet, Ulrich and Weller, Adrian and Winther, Ole and Ruozzi, Nicholas},
	file = {paquet.pdf:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\EUG7PBKT\\paquet.pdf:application/pdf}
}

@article{quinonero-candela_unifying_2005,
	title = {A unifying view of sparse approximate {Gaussian} process regression},
	volume = {6},
	url = {http://www.jmlr.org/papers/v6/quinonero-candela05a.html},
	number = {Dec},
	urldate = {2017-05-23},
	journal = {Journal of Machine Learning Research},
	author = {Quiñonero-Candela, Joaquin and Rasmussen, Carl Edward},
	year = {2005},
	pages = {1939--1959},
	file = {quinonero-candela05a.pdf:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\8MXVM9Q8\\quinonero-candela05a.pdf:application/pdf}
}

@article{bauer_understanding_2016,
	title = {Understanding {Probabilistic} {Sparse} {Gaussian} {Process} {Approximations}},
	url = {http://arxiv.org/abs/1606.04820},
	abstract = {Good sparse approximations are essential for practical inference in Gaussian Processes as the computational cost of exact methods is prohibitive for large datasets. The Fully Independent Training Conditional (FITC) and the Variational Free Energy (VFE) approximations are two recent popular methods. Despite superficial similarities, these approximations have surprisingly different theoretical properties and behave differently in practice. We thoroughly investigate the two methods for regression both analytically and through illustrative examples, and draw conclusions to guide practical application.},
	urldate = {2017-05-23},
	journal = {arXiv:1606.04820 [stat]},
	author = {Bauer, Matthias Stephan and van der Wilk, Mark and Rasmussen, Carl Edward},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.04820},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1606.04820 PDF:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\4BWREIKP\\Bauer et al. - 2016 - Understanding Probabilistic Sparse Gaussian Proces.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\ITG6CK5W\\1606.html:text/html}
}

@article{hernandez-lobato_black-box_2016,
	title = {Black-box α-divergence minimization},
	url = {http://www.jmlr.org/proceedings/papers/v48/hernandez-lobatob16.pdf},
	urldate = {2017-05-23},
	author = {Hernández-Lobato, José Miguel and Li, Yingzhen and Rowland, Mark and Hernández-Lobato, Daniel and Bui, Thang D. and Turner, Richard E.},
	year = {2016},
	file = {hernandez-lobatob16.pdf:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\JCJMZVKN\\hernandez-lobatob16.pdf:application/pdf}
}

@inproceedings{li_renyi_2016,
	title = {Rényi divergence variational inference},
	url = {http://papers.nips.cc/paper/6207-renyi-divergence-variational-inference},
	urldate = {2017-05-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Li, Yingzhen and Turner, Richard E.},
	year = {2016},
	pages = {1073--1081},
	file = {6208-renyi-divergence-variational-inference.pdf:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\AXAZ45G5\\6208-renyi-divergence-variational-inference.pdf:application/pdf}
}

@book{thrun_learning_1998,
	title = {Learning to {Learn}},
	isbn = {978-1-4615-5529-2},
	abstract = {Over the past three decades or so, research on machine learning and data mining has led to a wide variety of algorithms that learn general functions from experience. As machine learning is maturing, it has begun to make the successful transition from academic research to various practical applications. Generic techniques such as decision trees and artificial neural networks, for example, are now being used in various commercial and industrial applications.  Learning to Learn is an exciting new research direction within machine learning. Similar to traditional machine-learning algorithms, the methods described in Learning to Learn induce general functions from experience. However, the book investigates algorithms that can change the way they generalize, i.e., practice the task of learning itself, and improve on it.  To illustrate the utility of learning to learn, it is worthwhile comparing machine learning with human learning. Humans encounter a continual stream of learning tasks. They do not just learn concepts or motor skills, they also learn bias, i.e., they learn how to generalize. As a result, humans are often able to generalize correctly from extremely few examples - often just a single example suffices to teach us a new thing.  A deeper understanding of computer programs that improve their ability to learn can have a large practical impact on the field of machine learning and beyond. In recent years, the field has made significant progress towards a theory of learning to learn along with practical new algorithms, some of which led to impressive results in real-world applications.  Learning to Learn provides a survey of some of the most exciting new research approaches, written by leading researchers in the field. Its objective is to investigate the utility and feasibility of computer programs that can learn how to learn, both from a practical and a theoretical point of view.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Thrun, Sebastian and Pratt, Lorien},
	year = {1998},
	note = {Google-Books-ID: X\_jpBwAAQBAJ},
	keywords = {Computers / Intelligence (AI) \& Semantics, Computers / Software Development \& Engineering / General}
}

@article{andrychowicz_learning_2016,
	title = {Learning to learn by gradient descent by gradient descent},
	url = {http://arxiv.org/abs/1606.04474},
	abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
	journal = {arXiv:1606.04474 [cs]},
	author = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and Shillingford, Brendan and de Freitas, Nando},
	month = nov,
	year = {2016},
	note = {arXiv: 1606.04474},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1606.04474 PDF:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\423HAX9U\\Andrychowicz et al. - 2016 - Learning to learn by gradient descent by gradient .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\Z5SAB6GE\\1606.html:text/html}
}

@misc{noauthor_patricia_learning_to_learn_2014_cvpr_paper.pdf_nodate,
	title = {Patricia\_Learning\_to\_Learn\_2014\_CVPR\_paper.pdf},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Patricia_Learning_to_Learn_2014_CVPR_paper.pdf},
	urldate = {2017-10-13},
	file = {Patricia_Learning_to_Learn_2014_CVPR_paper.pdf:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\PTIMD745\\Patricia_Learning_to_Learn_2014_CVPR_paper.pdf:application/pdf}
}

@inproceedings{patricia_learning_2014,
	title = {Learning to {Learn}, from {Transfer} {Learning} to {Domain} {Adaptation}: {A} {Unifying} {Perspective}},
	shorttitle = {Learning to {Learn}, from {Transfer} {Learning} to {Domain} {Adaptation}},
	doi = {10.1109/CVPR.2014.187},
	abstract = {The transfer learning and domain adaptation problems originate from a distribution mismatch between the source and target data distribution. The causes of such mismatch are traditionally considered different. Thus, transfer learn- ing and domain adaptation algorithms are designed to ad- dress different issues, and cannot be used in both settings unless substantially modified. Still, one might argue that these problems are just different declinations of learning to learn, i.e. the ability to leverage over prior knowledge when attempting to solve a new task. We propose a learning to learn framework able to lever- age over source data regardless of the origin of the distri- bution mismatch. We consider prior models as experts, and use their output confidence value as features. We use them to build the new target model, combined with the features from the target data through a high-level cue integration scheme. This results in a class of algorithms usable in a plug-and-play fashion over any learning to learn scenario, from binary and multi-class transfer learning to single and multiple source domain adaptation settings. Experiments on several public datasets show that our approach consis- tently achieves the state of the art.},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Patricia, N. and Caputo, B.},
	month = jun,
	year = {2014},
	keywords = {Adaptation models, Algorithm design and analysis, Benchmark testing, distribution mismatch, domain adaptation problem, Kernel, learning (artificial intelligence), learning to learn framework, Support vector machines, Training, transfer learning, Vectors},
	pages = {1442--1449},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\8K9XJEMS\\6909583.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\VXJ73VSR\\Patricia and Caputo - 2014 - Learning to Learn, from Transfer Learning to Domai.pdf:application/pdf}
}

@article{dwork_algorithmic_2014,
	title = {The {Algorithmic} {Foundations} of {Differential} {Privacy}},
	volume = {9},
	issn = {1551-305X, 1551-3068},
	url = {http://www.nowpublishers.com/articles/foundations-and-trends-in-theoretical-computer-science/TCS-042},
	doi = {10.1561/0400000042},
	language = {en},
	number = {3-4},
	urldate = {2017-10-18},
	journal = {Foundations and Trends® in Theoretical Computer Science},
	author = {Dwork, Cynthia and Roth, Aaron},
	year = {2014},
	pages = {211--407},
	file = {privacybook.pdf:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\5TNKA7B7\\privacybook.pdf:application/pdf}
}

@article{gelman_expectation_2014,
	title = {Expectation propagation as a way of life: {A} framework for {Bayesian} inference on partitioned data},
	shorttitle = {Expectation propagation as a way of life},
	url = {http://arxiv.org/abs/1412.4869},
	abstract = {A common approach for Bayesian computation with big data is to partition the data into smaller pieces, perform local inference for each piece separately, and finally combine the results to obtain an approximation to the global posterior. Looking at this from the bottom up, one can perform separate analyses on individual sources of data and then want to combine these in a larger Bayesian model. In either case, the idea of distributed modeling and inference has both conceptual and computational appeal, but from the Bayesian perspective there is no general way of handling the prior distribution: if the prior is included in each separate inference, it will be multiply-counted when the inferences are combined; but if the prior is itself divided into pieces, it may not provide enough regularization for each separate computation, thus eliminating one of the key advantages of Bayesian methods. To resolve this dilemma, expectation propagation (EP) has been proposed as a prototype for distributed Bayesian inference. The central idea is to factor the likelihood according to the data partitions, and to iteratively combine each factor with an approximate model of the prior and all other parts of the data, thus producing an overall approximation to the global posterior at convergence. In this paper, we give an introduction to EP and an overview of some recent developments of the method, with particular emphasis on its use in combining inferences from partitioned data. In addition to distributed modeling of large datasets, our unified treatment also includes hierarchical modeling of data with a naturally partitioned structure.},
	journal = {arXiv:1412.4869 [stat]},
	author = {Gelman, Andrew and Vehtari, Aki and Jylänki, Pasi and Sivula, Tuomas and Tran, Dustin and Sahai, Swupnil and Blomstedt, Paul and Cunningham, John P. and Schiminovich, David and Robert, Christian},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.4869},
	keywords = {Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv\:1412.4869 PDF:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\4X9WCHJQ\\Gelman et al. - 2014 - Expectation propagation as a way of life A framew.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\K8G896G6\\1412.html:text/html}
}

@incollection{li_stochastic_2015,
title = {Stochastic Expectation Propagation},
author = {Li, Yingzhen and Hern\'{a}ndez-Lobato, Jos\'{e} Miguel and Turner, Richard E},
booktitle = {Advances in Neural Information Processing Systems 28},
pages = {2323--2331},
year = {2015},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5760-stochastic-expectation-propagation.pdf}
}


@article{opper_theory_2016,
	title = {A {Theory} of {Solving} {TAP} {Equations} for {Ising} {Models} with {General} {Invariant} {Random} {Matrices}},
	volume = {49},
	issn = {1751-8113, 1751-8121},
	url = {http://arxiv.org/abs/1509.01229},
	doi = {10.1088/1751-8113/49/11/114002},
	abstract = {We consider the problem of solving TAP mean field equations by iteration for Ising model with coupling matrices that are drawn at random from general invariant ensembles. We develop an analysis of iterative algorithms using a dynamical functional approach that in the thermodynamic limit yields an effective dynamics of a single variable trajectory. Our main novel contribution is the expression for the implicit memory term of the dynamics for general invariant ensembles. By subtracting these terms, that depend on magnetizations at previous time steps, the implicit memory terms cancel making the iteration dependent on a Gaussian distributed field only. The TAP magnetizations are stable fixed points if an AT stability criterion is fulfilled. We illustrate our method explicitly for coupling matrices drawn from the random orthogonal ensemble.},
	number = {11},
	journal = {Journal of Physics A: Mathematical and Theoretical},
	author = {Opper, Manfred and Çakmak, Burak and Winther, Ole},
	month = mar,
	year = {2016},
	note = {arXiv: 1509.01229},
	keywords = {Computer Science - Information Theory, Condensed Matter - Disordered Systems and Neural Networks},
	pages = {114002},
	file = {arXiv\:1509.01229 PDF:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\RXWM5MG9\\Opper et al. - 2016 - A Theory of Solving TAP Equations for Ising Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\PJUHGZEW\\1509.html:text/html}
}

@article{csato_sparse_2002,
	title = {Sparse {On}-{Line} {Gaussian} {Processes}},
	volume = {14},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/089976602317250933},
	doi = {10.1162/089976602317250933},
	number = {3},
	journal = {Neural Computation},
	author = {Csató, Lehel and Opper, Manfred},
	month = mar,
	year = {2002},
	pages = {641--668},
	file = {Neural Computation Full Text PDF:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\8TTNBGBE\\Csató and Opper - 2002 - Sparse On-Line Gaussian Processes.pdf:application/pdf;Neural Computation Snapshot:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\ATH42HWP\\089976602317250933.html:text/html}
}

@article{turner_two_2011,
	title = {Two problems with variational expectation maximisation for time-series models},
	journal = {Bayesian Time series models},
	author = {Turner, Richard E. and Sahani, Maneesh},
	year = {2011},
	pages = {115--138},
	file = {turner-and-sahani-2011a.pdf:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\336P7Z37\\turner-and-sahani-2011a.pdf:application/pdf}
}

@misc{siddharthswaroop_contribute_2017,
	title = {Contribute to masters-project development by creating an account on {GitHub}},
	url = {https://github.com/siddharthswaroop/masters-project},
	author = {siddharthswaroop},
	month = oct,
	year = {2017},
	note = {original-date: 2017-10-31T13:17:20Z},
	file = {Snapshot:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\M6U9Z727\\Swaroop 2017.html:text/html}
}

@misc{siddharthswaroop_contribute_2017-1,
	title = {Contribute to masters-project development by creating an account on {GitHub}},
	url = {https://github.com/siddharthswaroop/masters-project},
	author = {siddharthswaroop},
	month = oct,
	year = {2017},
	note = {original-date: 2017-10-31T13:17:20Z},
	file = {Snapshot:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\XP743GZH\\Swaroop 2017.html:text/html}
}

@techreport{swaroop_understanding_2017,
	title = {Understanding {Expectation} {Propagation}},
	url = {https://github.com/siddharthswaroop/masters-project/blob/master/Swaroop%202017.pdf},
	urldate = {2017-10-31},
	institution = {University of Cambridge},
	author = {Swaroop, Siddharth},
	month = may,
	year = {2017}
}

@incollection{dehaene_bounding_2015,
	title = {Bounding errors of {Expectation}-{Propagation}},
	url = {http://papers.nips.cc/paper/5912-bounding-errors-of-expectation-propagation.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Dehaene, Guillaume P and Barthelmé, Simon},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {244--252},
	file = {NIPS Full Text PDF:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\GIH2K7J8\\Dehaene and Barthelmé - 2015 - Bounding errors of Expectation-Propagation.pdf:application/pdf;NIPS Snapshort:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\IPRJNQ7U\\5912-bounding-errors-of-expectation-propagation.html:text/html}
}

@article{bui_unifying_2017,
	title = {A {Unifying} {Framework} for {Gaussian} {Process} {Pseudo}-{Point} {Approximations} using {Power} {Expectation} {Propagation}},
	volume = {18},
	url = {http://jmlr.org/papers/v18/16-603.html},
	number = {104},
	urldate = {2017-11-01},
	journal = {Journal of Machine Learning Research},
	author = {Bui, Thang D. and Yan, Josiah and Turner, Richard E.},
	year = {2017},
	pages = {1--72},
	file = {Full Text PDF:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\HNIC3T44\\Bui et al. - 2017 - A Unifying Framework for Gaussian Process Pseudo-P.pdf:application/pdf;Snapshot:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\3EG8HQA2\\16-603.html:text/html}
}

@incollection{herbrich_trueskill_2007,
	title = {{TrueSkill}™ : {A} {Bayesian} {Skill} {Rating} {System}},
	shorttitle = {{TrueSkill}™},
	url = {http://papers.nips.cc/paper/3079-trueskilltm-a-bayesian-skill-rating-system.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 19},
	publisher = {MIT Press},
	author = {Herbrich, Ralf and Minka, Tom and Graepel, Thore},
	editor = {Schölkopf, P. B. and Platt, J. C. and Hoffman, T.},
	year = {2007},
	pages = {569--576},
	file = {NIPS Full Text PDF:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\PIJMTDD4\\Herbrich et al. - 2007 - TrueSkill™  A Bayesian Skill Rating System.pdf:application/pdf;NIPS Snapshort:C\:\\Users\\Siddharth\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\30vvkkve.default\\zotero\\storage\\TEKF3EQN\\3079-trueskilltm-a-bayesian-skill-rating-system.html:text/html}
}

@misc{InferNET14, 
author = "Minka, T. and Winn, J.M. and Guiver, J.P. and Webster, S. and Zaykov, Y. and Yangel, B. and Spengler, A. and  Bronskill, J.",
title = {{Infer.NET 2.6}},
year = 2014,
note = {Microsoft Research Cambridge. http://research.microsoft.com/infernet}
}